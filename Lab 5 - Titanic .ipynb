{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Determining Who on the Titanic would have Survived Based on Age, Sex, and Class </h1>\n",
    "\n",
    "Ben Douglass\n",
    "\n",
    "12/6/16\n",
    "\n",
    "Big Data, Period 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Average accuracy of three types of Machine Learning Algorithms: DecisionTree, KNeighbors, and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2201, 4)\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#code to import a csv and convert it to a numpy array\n",
    "#import the numpy library\n",
    "import numpy as np\n",
    "#open the file\n",
    "raw_data = open(\"titanic.csv\")\n",
    "raw_data1 = open(\"titanic_no1.csv\")\n",
    "raw_data2 = open(\"titanic_no2.csv\")\n",
    "#load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "dataset1 = np.loadtxt(raw_data1, delimiter=\",\")\n",
    "dataset2 = np.loadtxt(raw_data2, delimiter=\",\")\n",
    "#'shape' is the dimensions of the matrix\n",
    "print(dataset.shape)\n",
    "# separate the data from the target attributes\n",
    "X = dataset[:,0:3]\n",
    "y = dataset[:,3]\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Average: 0.0\n",
      " \n",
      "0.780199818347\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Average: 0.0\n",
      " \n",
      "0.780199818347\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Average: 0.0\n",
      "0.128065395095\n"
     ]
    }
   ],
   "source": [
    "#My first try. I had some kind of issue that caused my scores to not be nearly high enough, so I tried another method.\n",
    "\n",
    "from sklearn import datasets\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier();\n",
    "tree_classifier = tree_classifier.fit(X_train, y_train)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "(accuracy_score(tree_predictions, y_test))\n",
    "\n",
    "for ii in range (10):\n",
    "    print (tree_predictions[ii])\n",
    "\n",
    "print (\"Average:\", ((tree_predictions[0] ++ tree_predictions[1] ++ tree_predictions[2] ++ tree_predictions[3] ++ tree_predictions[4] ++ tree_predictions[5] ++ tree_predictions[6] ++ tree_predictions[7] ++ tree_predictions[8] ++ tree_predictions[9])/10))\n",
    "\n",
    "\n",
    "print (\" \")\n",
    "\n",
    "from sklearn import neighbors\n",
    "knn_classifier = neighbors.KNeighborsClassifier()\n",
    "knn_classifier = knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(knn_predictions, y_test))\n",
    "\n",
    "for ii in range (10):\n",
    "    print (knn_predictions[ii])\n",
    "\n",
    "print (\"Average:\", ((knn_predictions[0] ++ knn_predictions[1] ++ knn_predictions[2] ++ knn_predictions[3] ++ knn_predictions[4] ++ knn_predictions[5] ++ knn_predictions[6] ++ knn_predictions[7] ++ knn_predictions[8] ++ knn_predictions[9])/10))\n",
    "\n",
    "\n",
    "print (\" \")\n",
    "\n",
    "from sklearn import ensemble\n",
    "rf_classifier = ensemble.RandomForestClassifier()\n",
    "rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "print (accuracy_score(rf_predictions, y_test))\n",
    "\n",
    "\n",
    "for ii in range (10):\n",
    "    print (rf_predictions[ii])\n",
    "\n",
    "print (\"Average:\", ((rf_predictions[0] ++ rf_predictions[1] ++ rf_predictions[2] ++ rf_predictions[3] ++ rf_predictions[4] ++ rf_predictions[5] ++ rf_predictions[6] ++ rf_predictions[7] ++ rf_predictions[8] ++ rf_predictions[9])/10))\n",
    "\n",
    "print(statistics.mean(knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78019981834695729, 0.77111716621253401, 0.76930063578564944, 0.79382379654859214, 0.76748410535876477, 0.78564940962761132, 0.7756584922797457, 0.79927338782924617, 0.78474114441416898, 0.78019981834695729]\n",
      "Average Accuracy of DecisionTree: 0.780744777475\n",
      "\n",
      "[0.77838328792007261, 0.57765667574931878, 0.56130790190735691, 0.77838328792007261, 0.78201634877384196, 0.77202543142597635, 0.78837420526793822, 0.79291553133514991, 0.75930971843778383, 0.79473206176203448]\n",
      "Average Accuracy of KNeighbors 0.73851044505\n",
      "\n",
      "[0.78019981834695729, 0.77111716621253401, 0.76930063578564944, 0.79382379654859214, 0.76748410535876477, 0.7756584922797457, 0.78837420526793822, 0.79654859218891916, 0.78474114441416898, 0.78019981834695729]\n",
      "Average Accuracy of RandomForest 0.780744777475\n"
     ]
    }
   ],
   "source": [
    "#Doc: Help from James\n",
    "#This is what I used to answer Question 1\n",
    "\n",
    "counter = 0 \n",
    "list_one = []\n",
    "list_two = []\n",
    "list_three = []\n",
    "\n",
    "while counter < 10:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "    \n",
    "    from sklearn import tree\n",
    "    tree_classifier = tree.DecisionTreeClassifier();\n",
    "    tree_classifier = tree_classifier.fit(X_train, y_train)\n",
    "    tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    data = (accuracy_score(tree_predictions, y_test))\n",
    "    list_one.append(data);\n",
    "    \n",
    "    \n",
    "    from sklearn import neighbors\n",
    "    knn_classifier = neighbors.KNeighborsClassifier()\n",
    "    knn_classifier = knn_classifier.fit(X_train, y_train)\n",
    "    knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "    data2 = (accuracy_score(knn_predictions, y_test))\n",
    "    list_two.append(data2);\n",
    "    \n",
    "    \n",
    "    from sklearn import ensemble\n",
    "    rf_classifier = ensemble.RandomForestClassifier()\n",
    "    rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "    rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "    data3 = (accuracy_score(rf_predictions, y_test))\n",
    "    list_three.append(data3)\n",
    "    \n",
    "    counter+=1\n",
    "    \n",
    "print(list_one)\n",
    "print(\"Average Accuracy of DecisionTree:\", statistics.mean(list_one))\n",
    "print ( )\n",
    "print(list_two)\n",
    "print(\"Average Accuracy of KNeighbors\", statistics.mean(list_two))\n",
    "print ( )\n",
    "print(list_three)\n",
    "print(\"Average Accuracy of RandomForest\", statistics.mean(list_three))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 1 Answer: </h3>\n",
    "\n",
    "I found that the third algorithm, <b>RandomForest</b>, was the most accurate because it has the highest accuracy score of 78.8. However, the other two machine learning algorithms were close behind, with scores of 78.6 from DecisionTree and 77.0 from KNeighbors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Using the most accurate algorithm, determine whether the following people would have survived:\n",
    "\n",
    "\n",
    "an adult male in 3rd class, an adult female in 1st class, and a female child in 1st class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = dead\n",
      "1 = alive\n",
      "[ 0.]\n",
      "[ 1.]\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "rf_classifier = ensemble.RandomForestClassifier()\n",
    "rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"0 = dead\")\n",
    "print(\"1 = alive\")\n",
    "\n",
    "\n",
    "rf_predictions = rf_classifier.predict([[3, 1, 1]])\n",
    "print (rf_predictions)\n",
    "\n",
    "rf_predictions = rf_classifier.predict([[1, 1, 0]])\n",
    "print (rf_predictions)\n",
    "\n",
    "rf_predictions = rf_classifier.predict([[1, 0, 0]])\n",
    "print (rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 2 Answer: </h3>\n",
    "\n",
    "I used the RandomForest algorithm, which I found in the previous question was the most accurate. \n",
    "\n",
    "<u>Here is who lived and who died:</u>\n",
    "\n",
    "The adult male in 3rd class most likely <b>died</b>\n",
    "\n",
    "The adult female in 1st class most likely <b>lived</b>\n",
    "\n",
    "the female child in 1st class most likely <b>lived</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Which is the most important feature: Age, Class, or Gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[0.76839237057220711, 0.77656675749318804, 0.78110808356039962, 0.76203451407811085, 0.76475930971843775, 0.76930063578564944, 0.76657584014532243, 0.78019981834695729, 0.75567665758401448, 0.76839237057220711]\n",
      "Average Accuracy of DecisionTree: 0.769300635786\n",
      "\n",
      "[0.76657584014532243, 0.77202543142597635, 0.77747502270663038, 0.77838328792007261, 0.77656675749318804, 0.77293369663941869, 0.76657584014532243, 0.78019981834695729, 0.74750227066303365, 0.77929155313351495]\n",
      "Average Accuracy of KNeighbors 0.771752951862\n",
      "\n",
      "[0.76839237057220711, 0.77656675749318804, 0.78110808356039962, 0.76839237057220711, 0.76021798365122617, 0.76930063578564944, 0.76657584014532243, 0.78019981834695729, 0.75567665758401448, 0.76839237057220711]\n",
      "Average Accuracy of RandomForest 0.769482288828\n"
     ]
    }
   ],
   "source": [
    "#I initially tried to create different excell files with one feature removed and see what the accuracy scores were.\n",
    "#However, this method didn't work because for some reason the accuracy scores did not change very much. \n",
    "#So I did a different method. \n",
    "\n",
    "X = dataset1[:,0:2]\n",
    "y = dataset1[:,2]\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "counter = 0 \n",
    "list_one = []\n",
    "list_two = []\n",
    "list_three = []\n",
    "\n",
    "while counter < 10:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "    \n",
    "    from sklearn import tree\n",
    "    tree_classifier = tree.DecisionTreeClassifier();\n",
    "    tree_classifier = tree_classifier.fit(X_train, y_train)\n",
    "    tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    data = (accuracy_score(tree_predictions, y_test))\n",
    "    list_one.append(data);\n",
    "    \n",
    "    \n",
    "    from sklearn import neighbors\n",
    "    knn_classifier = neighbors.KNeighborsClassifier()\n",
    "    knn_classifier = knn_classifier.fit(X_train, y_train)\n",
    "    knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "    data2 = (accuracy_score(knn_predictions, y_test))\n",
    "    list_two.append(data2);\n",
    "    \n",
    "    \n",
    "    from sklearn import ensemble\n",
    "    rf_classifier = ensemble.RandomForestClassifier()\n",
    "    rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "    rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "    data3 = (accuracy_score(rf_predictions, y_test))\n",
    "    list_three.append(data3)\n",
    "    \n",
    "    counter = counter +1\n",
    "    \n",
    "print(list_one)\n",
    "print(\"Average Accuracy of DecisionTree:\", statistics.mean(list_one))\n",
    "print ( )\n",
    "print(list_two)\n",
    "print(\"Average Accuracy of KNeighbors\", statistics.mean(list_two))\n",
    "print ( )\n",
    "print(list_three)\n",
    "print(\"Average Accuracy of RandomForest\", statistics.mean(list_three))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " ..., \n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[0.78383287920072664, 0.77202543142597635, 0.77384196185286103, 0.76203451407811085, 0.78564940962761132, 0.79564032697547682, 0.76385104450499541, 0.78564940962761132, 0.78928247048138056, 0.76475930971843775]\n",
      "Average Accuracy of DecisionTree: 0.777656675749\n",
      "\n",
      "[0.78383287920072664, 0.56494096276112626, 0.77384196185286103, 0.74114441416893728, 0.56494096276112626, 0.79564032697547682, 0.78383287920072664, 0.77384196185286103, 0.78928247048138056, 0.75386012715712991]\n",
      "Average Accuracy of KNeighbors 0.732515894641\n",
      "\n",
      "[0.78383287920072664, 0.77202543142597635, 0.77384196185286103, 0.76203451407811085, 0.78564940962761132, 0.78474114441416898, 0.76385104450499541, 0.78564940962761132, 0.78928247048138056, 0.76475930971843775]\n",
      "Average Accuracy of RandomForest 0.776566757493\n"
     ]
    }
   ],
   "source": [
    "X = dataset2[:,0:2]\n",
    "y = dataset2[:,2]\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "counter = 0 \n",
    "list_one = []\n",
    "list_two = []\n",
    "list_three = []\n",
    "\n",
    "while counter < 10:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "    \n",
    "    from sklearn import tree\n",
    "    tree_classifier = tree.DecisionTreeClassifier();\n",
    "    tree_classifier = tree_classifier.fit(X_train, y_train)\n",
    "    tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    data = (accuracy_score(tree_predictions, y_test))\n",
    "    list_one.append(data);\n",
    "    \n",
    "    \n",
    "    from sklearn import neighbors\n",
    "    knn_classifier = neighbors.KNeighborsClassifier()\n",
    "    knn_classifier = knn_classifier.fit(X_train, y_train)\n",
    "    knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "    data2 = (accuracy_score(knn_predictions, y_test))\n",
    "    list_two.append(data2);\n",
    "    \n",
    "    \n",
    "    from sklearn import ensemble\n",
    "    rf_classifier = ensemble.RandomForestClassifier()\n",
    "    rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "    rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "    data3 = (accuracy_score(rf_predictions, y_test))\n",
    "    list_three.append(data3)\n",
    "    \n",
    "    counter = counter +1\n",
    "    \n",
    "print(list_one)\n",
    "print(\"Average Accuracy of DecisionTree:\", statistics.mean(list_one))\n",
    "print ( )\n",
    "print(list_two)\n",
    "print(\"Average Accuracy of KNeighbors\", statistics.mean(list_two))\n",
    "print ( )\n",
    "print(list_three)\n",
    "print(\"Average Accuracy of RandomForest\", statistics.mean(list_three))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DOC: Andrew\n",
    "#Here is the method I used.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from math import sqrt\n",
    "\n",
    "raw_data = open(\"titanic.csv\");\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\");\n",
    "\n",
    "X = dataset[:,0:3];\n",
    "y = dataset[:,3];\n",
    "\n",
    "def neednum():\n",
    "    mmn = input(\"Please enter a number between 0 and 3: \");\n",
    "    try:\n",
    "        cnum = int(mmn);\n",
    "        Class = cnum;\n",
    "        if cnum > 3:\n",
    "            neednum();\n",
    "        return Class;\n",
    "    except:\n",
    "        neednum();\n",
    "\n",
    "Class = input(\"What class are you? (0-3, 0 being crew, 3 being third): \");\n",
    "cnum = 0;\n",
    "try:\n",
    "    cnum = int(Class);\n",
    "except:\n",
    "    neednum();\n",
    "if cnum > 3:\n",
    "    neednum();\n",
    "age = input(\"Are you an adult or child? (A/C): \");\n",
    "sex = input(\"What sex are you? (M/F): \");\n",
    "\n",
    "age = age.lower();\n",
    "sex = sex.lower();\n",
    "\n",
    "if age == \"a\":\n",
    "    age = 1;\n",
    "elif age == \"c\":\n",
    "    age = 0;\n",
    "\n",
    "if sex == \"m\":\n",
    "    sex = 1;\n",
    "elif sex == \"f\":\n",
    "    sex = 0;\n",
    "\n",
    "final = [[cnum, age, sex]];\n",
    "print(final);\n",
    "check1 = 0;\n",
    "check2 = 0;\n",
    "check3 = 0;\n",
    "check4 = 0;\n",
    "check5 = 0;\n",
    "check6 = 0;\n",
    "check7 = 0;\n",
    "check8 = 0;\n",
    "check9 = 0;\n",
    "check10 = 0;\n",
    "\n",
    "inp = input(\"How many random samples would you like? \");\n",
    "inp4 = input(\"Show slice info? (Not recommended for large tests): \");\n",
    "inp4 = inp4.lower();\n",
    "inp5 = input(\"Risk crash? (Y/N): \");\n",
    "inp6 = input(\"Show instant results?: \");\n",
    "inp5 = inp5.lower();\n",
    "inp6 = inp6.lower();\n",
    "if inp4 == \"yes\" or inp4 == \"yeah\" or inp4 == \"yep\" or inp4 == \"y\":\n",
    "    inp4 = \"y\";\n",
    "if inp6 == \"yes\" or inp6 == \"yeah\" or inp6 == \"yep\" or inp6 == \"y\":\n",
    "    inp6 = \"y\";\n",
    "inp2 = int(inp);\n",
    "\n",
    "sSizes = [];\n",
    "iterator = 0;\n",
    "\n",
    "treeList = [];\n",
    "nearList = [];\n",
    "forestList = [];\n",
    "uList = [];\n",
    "\n",
    "hour = 0;\n",
    "hours = 0;\n",
    "minute = 0;\n",
    "minutes = 0;\n",
    "seconds = 0;\n",
    "\n",
    "start_time1 = time.time();#Time to make some efficiency calculations\n",
    "while iterator < inp2:\n",
    "    samp = np.random.rand();\n",
    "    sSizes.append(samp);\n",
    "    iterator+=1;\n",
    "print(\"%s seconds to generate slices.\" % (time.time() - start_time1));\n",
    "\n",
    "start_time2 = time.time();#Give user view of how long things are gonna take from a rough sample of generating 10%\n",
    "start_time3 = time.time();#Metric to measure by\n",
    "\n",
    "def timing(stime):#ETA Calc (old)\n",
    "    print((a/len(sSizes)))\n",
    "    try:\n",
    "        hour = int((time.time()-stime)/(a/len(sSizes))-(time.time()-stime))/60/60;\n",
    "    except:\n",
    "        hour = 0;\n",
    "    hours = int(hour)\n",
    "    try:\n",
    "        minute = int((time.time()-stime)/(a/len(sSizes))-(time.time()-stime))/60;\n",
    "    except:\n",
    "        minute = 0;\n",
    "    minutes = int(minute);\n",
    "    while minutes > 59:\n",
    "        minutes = minutes-60;\n",
    "    seconds = int((time.time()-stime)/(a/len(sSizes))-(time.time()-stime))%60;\n",
    "    if hours == 1 and minutes > 1:\n",
    "        print(\"ETA\", hours, \"hour,\", minutes, \"minutes and\", seconds, \"seconds to completion at current rate.\");\n",
    "    elif minutes == 1 and hours > 1:\n",
    "        print(\"ETA\", hours, \"hours,\", minutes, \"minute and\", seconds, \"seconds to completion at current rate.\");\n",
    "    elif hours == 1 and minutes == 1:\n",
    "        print(\"ETA\", hour, \"hour,\", minute, \"minute and\", seconds, \"seconds to completion at current rate.\");\n",
    "    else:\n",
    "        print(\"ETA\", hours, \"hours,\", minutes, \"minutes and\", seconds, \"seconds to completion at current rate.\");\n",
    "    print();\n",
    "\n",
    "#Loading bar equipment\n",
    "d = 0;\n",
    "e = 100;\n",
    "bar = \"¶\";\n",
    "blank = \"¶\";\n",
    "\n",
    "st4 = time.time();\n",
    "\n",
    "for a in range(len(sSizes)):\n",
    "    if inp5 == \"y\":\n",
    "        b = float(a/len(sSizes)*100);\n",
    "        c = str(b)+\"%\";\n",
    "        perc = c+\" done.\";\n",
    "        if int(b) != d:\n",
    "            d = int(b);\n",
    "            e-=1;\n",
    "        physbar = \"¶\"+str(bar*d)+str(blank*e)+\"¶\";\n",
    "        print(\"\\r\", physbar, perc, end = \"\");\n",
    "    else:\n",
    "        if a/len(sSizes)*100 > 1.00 and check1 == 0:\n",
    "            print(\"1% done\");\n",
    "            timing(start_time3);\n",
    "            check1 = 1;\n",
    "        elif a/len(sSizes)*100 > 10.00 and check2 == 0:\n",
    "            print(\"10% done\");\n",
    "            print(\"%s seconds to process 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            timing(start_time3);\n",
    "            start_time2 = time.time();\n",
    "            check2 = 1;\n",
    "        elif a/len(sSizes)*100 > 20.00 and check3 == 0:\n",
    "            print(\"20% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            timing(start_time3);\n",
    "            start_time2 = time.time();\n",
    "            check3 = 1;\n",
    "        elif a/len(sSizes)*100 > 30.00 and check4 == 0:\n",
    "            print(\"30% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            timing(start_time3);\n",
    "            start_time2 = time.time();\n",
    "            check4 = 1;\n",
    "        elif a/len(sSizes)*100 > 40.00 and check5 == 0:\n",
    "            print(\"40% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            timing(start_time3);\n",
    "            check5 = 1;\n",
    "        elif a/len(sSizes)*100 > 50.00 and check6 == 0:\n",
    "            print(\"50% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            check6 = 1;\n",
    "            timing(start_time3);\n",
    "        elif a/len(sSizes)*100 > 60.00 and check7 == 0:\n",
    "            print(\"60% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            check7 = 1;\n",
    "            timing(start_time3);\n",
    "        elif a/len(sSizes)*100 > 70.00 and check8 == 0:\n",
    "            print(\"70% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            check8 = 1;\n",
    "            timing(start_time3);\n",
    "        elif a/len(sSizes)*100 > 80.00 and check9 == 0:\n",
    "            print(\"80% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            check9 = 1;\n",
    "            timing(start_time3);\n",
    "        elif a/len(sSizes)*100 > 90.00 and check10 == 0:\n",
    "            print(\"90% done\");\n",
    "            print(\"%s seconds to process another 10 percent.\" % (int(time.time() - start_time2)));\n",
    "            start_time2 = time.time();\n",
    "            check10 = 1;\n",
    "            timing(start_time3);\n",
    "    if inp4 == \"y\":\n",
    "        print(\"Entry\", a+1, \"has a sample partition of:\", sSizes[a]);\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = sSizes[a]);\n",
    "    \n",
    "    try:\n",
    "        tree_classifier = tree.DecisionTreeClassifier();\n",
    "        tree_classifier = tree_classifier.fit(X_train, y_train);\n",
    "        tree_predictions = tree_classifier.predict(final);\n",
    "    except:\n",
    "        if inp6 == \"y\":\n",
    "            print(\"Bad data at sample\", a+1,\"with slice value of:\", sSizes[a],\"for Tree method\");\n",
    "    try:\n",
    "        knn_classifier = neighbors.KNeighborsClassifier();\n",
    "        knn_classifier = knn_classifier.fit(X_train, y_train);\n",
    "        knn_predictions = knn_classifier.predict(final);\n",
    "    except:\n",
    "        if inp6 == \"y\":\n",
    "            print(\"Bad data at sample\", a+1,\"with slice value of:\", sSizes[a],\"for Neighbor method\");\n",
    "        \n",
    "    try:\n",
    "        rf_classifier = ensemble.RandomForestClassifier();\n",
    "        rf_classifier = rf_classifier.fit(X_train, y_train);\n",
    "        rf_predictions = rf_classifier.predict(final);\n",
    "    except:\n",
    "        if inp6 == \"y\":\n",
    "            print(\"Bad data at sample\", a+1,\"with slice value of:\", sSizes[a],\"for Forest method\");\n",
    "    \n",
    "    if inp6 == \"y\":\n",
    "        print(\"Tree:\",tree_predictions);\n",
    "        print(\"Nearest:\", knn_predictions);\n",
    "        print(\"Forest:\", rf_predictions);\n",
    "    \n",
    "    treeList.append(tree_predictions);\n",
    "    nearList.append(knn_predictions);\n",
    "    forestList.append(rf_predictions);\n",
    "print();\n",
    "print(\"Finished in %s seconds.\" % (int(time.time()-start_time3)));\n",
    "print(\"Tree avg % chance of survival:\",np.mean(treeList)*100);\n",
    "print(\"Nearest avg % chance of survival:\",np.mean(nearList)*100);\n",
    "print(\"Forest avg % chance of survival:\",np.mean(forestList)*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Question 3 Answer:</h3>\n",
    "\n",
    "To find which feature was the most important in determining whether a person on the Titanic lived or not, I iterated through many combinations of features to find trends.\n",
    "\n",
    "Here are the combinations I tried:\n",
    "\n",
    "Variations going through:\n",
    "\n",
    "    0, A, M -- Forest = 0.0\n",
    "    3, A, M -- Forest = 0.2\n",
    "    2, A, M -- Forest = 0.4\n",
    "    1, A, M -- Forest = 2.8\n",
    "    \n",
    "    0, A, F -- Forest = 96.8\n",
    "    3, A, F -- Forest = 19.2\n",
    "    2, A, F -- Forest = 99.4\n",
    "    1, A, F -- Forest = 99.6\n",
    "    \n",
    "    0, C, M -- Forest = 87.8\n",
    "    3, C, M -- Forest = 3.4\n",
    "    2, C, M -- Forest = 95.0\n",
    "    1, C, M -- Forest = 97.2\n",
    "    \n",
    "    0, C, F -- Forest = 99.0\n",
    "    3, C, F -- Forest = 28.2\n",
    "    2, C, F -- Forest = 98.6\n",
    "    1, C, F -- Forest = 99.2\n",
    "    \n",
    "    *A = Adult\n",
    "    *C = Child\n",
    "    *M = Male\n",
    "    *F = Female\n",
    "    *0-3 = Classes\n",
    "    \n",
    "For all of these, I used 500 samples to get accurate scores that would not take longer than 10 seconds to load each time. This amount of samples gets my number down to the nearest .2, which is helpful in making it more accurate while still being reasonable in load time. \n",
    "\n",
    "<u>Here are some of my conclusions from testing all these combinations. </u>\n",
    "    \n",
    "Class - This feature mattered mostly whether the person was in 3rd class or not. Besides 3rd class, females of all ages and male children were likely to live. Males of all classes were extremely likely to die, but 1st class men had a slight chance of living. \n",
    "\n",
    "Gender - In adults, gender made an extreme difference. Adult females are very likely to live (besides third class), whereas males were highly likely to die. In children, gender did not make as much of a difference. Boys were only slightly less likely to live than girls. It was consistent overall that females were more likely to live than males. \n",
    "\n",
    "Age - In females, the likelihood of surviving was about the same regardless of age. The only slight difference in female adults vs. female children was that female adults in higher classes were slightly more likely to survive, and female children in working/lower classes were more likely to survive. As for males, being a child vs. adult made a huge difference. Male children were likely to live and male adults were highly likely to die. Among all ages and gender combinations, being third class meant you were likely to die. \n",
    "    \n",
    "Each of these features made a significant difference in determining who lived or died. I believe the least important feature of the three was Class, because all but one tended to be likely to live. First and second class tended to be almost the same, with the workers on the Titanic right behind them, and third class people usually died. <b>I think the most important feature was Gender. </b> Among all classes and ages, gender made a difference. Though this difference was smaller in childen overall, what pushed me to say gender over age is that a working female child had a 99.0% chance of living, whereas a working male child had a 87.8% chance of living, and in third class a female child had a 28.2% chance of living, whereas a male child in third class had a 3.4% chance of living. This is still a big difference, even thought it's not as extreme as the difference between the survivial rates of male and female adults. Age was a close second in how much it influenced the outcome. \n",
    "\n",
    "\n",
    " <h3> Conclusion </h3>\n",
    " \n",
    "In this project, I learned about how to test the accuracy of machine learning algorithms, as well as how to use them to get useful results from a set of data. It is interesting how a computer is able to figure out patterns and can use this to get results when given parameters about a dataset, without me ever directly telling it the patterns. I felt that this was proven when the results of the patterns I discovered when testing different combinations of features matched the results I got for the three imaginary people I predicted would survive or die on the Titanic. The third class adult male died, and the first class adult and child females survived.\n",
    "\n",
    "I still have yet to grasp a better understanding of what these algorithms we use are like. By just calling them in the code, I do not actually see the math that goes on. I am curious about what the algorithm actually consists of mathematically. \n",
    "\n",
    "There are some aspects about the data that I think could have messed with my results a little bit. For example, I am concerned about how reasonable the results were for the children who worked on the titanic. There were no young children who worked on the titanic, but there were some teenagers. This also makes me wonder -- what was considered the cut-off age for children and adults in this data? This would affect the results for the surivial rates of the workers, as well as how age affected the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
